# Natural Language Processing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
# quoting = 3 means it will ignore all the "" in the text
dataset = pd.read_csv('Rating_list_2.csv')

# Cleaning the texts
import re
# toolkit to filter the unnecessary words 
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
# library used to clean each word (loved --> love)
from nltk.stem.porter import PorterStemmer
# a known word to describe a list of words after all the cleaning procces
corpus = []
for i in range(0, len(dataset)):
    print(i)
    # clean the comment so only the words remain (^ means stay)
    comment = re.sub('[^a-zA-Z]', ' ', dataset['Comment'][i])
    comment = comment.lower()
    # split the sentence to a list and filter it. we use set because its much faster
    comment = comment.split()
    ps = PorterStemmer()
    comment = [ps.stem(word) for word in comment if not word in set(stopwords.words('english'))]
    comment = ' '.join(comment)
    corpus.append(comment)

# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(max_features = 2000)
X = cv.fit_transform(corpus).toarray()
y = dataset.iloc[:, 1].values

'''from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Encoding the Dependent Variable
labelencoder_y = LabelEncoder()
y = labelencoder_y.fit_transform(y)'''

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Fitting Random Forest Classification to the Training set
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)


TP = cm[0][0]
FP = cm[0][1]
TN = cm[1][1]
FN = cm[1][0]
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
Score = 2 * Precision * Recall / (Precision + Recall)


from sklearn.externals import joblib 
# Save the model in a file 
filename = 'finalized_model.sav'
joblib.dump(classifier, filename)
filename = 'cv_file.sav'
joblib.dump(cv, filename)
